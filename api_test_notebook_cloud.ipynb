{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for the Zoho API\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import datetime as dt\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "\n",
    "class serpStackApi:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"http://api.serpstack.com/search\"\n",
    "        self.access_key = \"\"\n",
    "\n",
    "    def make_api_request(self, queries, location):\n",
    "        \"\"\"\n",
    "        Class to make the API Request.\n",
    "\n",
    "        Parameters:\n",
    "            - access_key: The Token ID is on Lambda enviroment \n",
    "            - query: The keyword for the Search\n",
    "            - location: The Location for the Search result \n",
    "            - language: The Language for the Search result\n",
    "        \"\"\"\n",
    "        all_responses = {}\n",
    "\n",
    "        for query in queries:\n",
    "            # Construct the API request URL\n",
    "            url = f\"{self.base_url}?access_key={self.access_key}&query={query}&auto_location=0&gl={location}&hl=en\"\n",
    "\n",
    "            try:\n",
    "                # Make the HTTP GET request\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful (status code 200)\n",
    "                if response.status_code == 200:\n",
    "                    # Parse JSON response\n",
    "                    json_response = response.json()\n",
    "\n",
    "                    # Add the query as a key and the response as a value in the dictionary\n",
    "                    all_responses[query] = json_response\n",
    "                else:\n",
    "                    # Print an error message if the request was not successful\n",
    "                    print(f\"Error: {response.status_code} - {response.text}\")\n",
    "                \n",
    "                # Sleep for 5 seconds before the next request\n",
    "                time.sleep(5)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle any exceptions that may occur during the request\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Iterate throught the API and the fields\n",
    "def save_results_to_csv(data):\n",
    "\n",
    "    # Define columns mapping for each result type\n",
    "    columns_mapping = {\n",
    "        'organic_results': {\n",
    "            'Position': 'position',\n",
    "            'Title': 'title',\n",
    "            'URL': 'url',\n",
    "            'Domain': 'domain',\n",
    "            'Displayed URL': 'displayed_url',\n",
    "            'Snippet': 'snippet',\n",
    "            'Cached Page URL': 'cached_page_url',\n",
    "            'Related Pages URL': 'related_pages_url'\n",
    "        },\n",
    "        'ads': {\n",
    "            'Position': 'position',\n",
    "            'Block Position': 'block_position',\n",
    "            'Title': 'title',\n",
    "            'URL': 'url',\n",
    "            'Tracking URL': 'tracking_url',\n",
    "            'Displayed URL': 'displayed_url',\n",
    "            'Description': 'description',\n",
    "            'Sitelinks': 'sitelinks'\n",
    "        },\n",
    "        'request': { \n",
    "            'Success': 'success',\n",
    "            'Processed Timestamp': 'processed_timestamp',\n",
    "            'Search URL': 'search_url',\n",
    "            'Total Time Taken': 'total_time_taken'\n",
    "        },\n",
    "        'search_parameters': { \n",
    "            'Engine': 'engine',\n",
    "            'Query': 'query',\n",
    "            'Type': 'type',\n",
    "            'Device': 'device',\n",
    "            'Google Domain': 'google_domain',\n",
    "            'HL': 'hl',\n",
    "            'GL': 'gl',\n",
    "            'Page': 'page',\n",
    "            'Num': 'num'\n",
    "        },\n",
    "        'search_information': { \n",
    "            'Total Results': 'total_results',\n",
    "            'Time Taken Displayed': 'time_taken_displayed',\n",
    "            'Did You Mean': 'did_you_mean',\n",
    "            'Showing Result For': 'showing_results_for',\n",
    "            'Query Displayed': 'query_displayed',\n",
    "            'Detected Location': 'detected_location',\n",
    "            'No Results for Original Query': 'no_results_for_original_query'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create a timestamp\n",
    "    timestamp = dt.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    current_date = dt.datetime.now()\n",
    "\n",
    "    # Iterate over the queries\n",
    "    for query_key in data:\n",
    "\n",
    "        # Iterate over the json result\n",
    "        for json_result_type, json_inner_schema in data[query_key].items():\n",
    "\n",
    "            # Iterate throught the expected dictionary structure\n",
    "            for given_result_type, given_inner_schema in columns_mapping.items():\n",
    "\n",
    "                # Check if the json_result matches the exected result_type\n",
    "                if json_result_type == given_result_type:\n",
    "\n",
    "                    # Create a DataFrame from the JSON data\n",
    "                    df = pd.DataFrame()\n",
    "\n",
    "                    # Evaluate if the JSON inner value is a Dictionary\n",
    "                    if isinstance(json_inner_schema, dict):\n",
    "\n",
    "                        # Iterate throught the JSON inners values\n",
    "                        for json_inner_key, json_inner_value in json_inner_schema.items():\n",
    "\n",
    "                            # Iterate throught the expected inner values\n",
    "                            for given_inner_key, given_inner_value in given_inner_schema.items():\n",
    "\n",
    "                                # Evaluate if the JSON inner column matches the expected json values\n",
    "                                if json_inner_key == given_inner_value:\n",
    "\n",
    "                                    df = df.append({'Query': query_key, 'Col': given_inner_key, 'Value': json_inner_value}, ignore_index=True)\n",
    "\n",
    "                        # Convert DataFrame to CSV string\n",
    "                        csv_content = df.to_csv(index=False)\n",
    "\n",
    "                        # Upload to S3\n",
    "                        s3 = boto3.client('s3')\n",
    "\n",
    "                        bucket_name = ''\n",
    "\n",
    "                        # Create the S3 key with the dynamic file prefix\n",
    "                        s3_key = f'eczack_capstone_datalake/staging_zone/serpstack_api/{json_result_type}/{current_date.year}/{current_date.month}/{current_date.day}/{json_result_type}_{timestamp}.csv'\n",
    "\n",
    "                        # Upload CSV content to S3\n",
    "                        s3.put_object(Body=csv_content, Bucket=bucket_name, Key=s3_key)\n",
    "\n",
    "                    # Evaluate if the JSON inner value is a List\n",
    "                    elif isinstance(json_inner_schema, list):\n",
    "\n",
    "                        for item in json_inner_schema:\n",
    "                            # Iterate throught the expected inner values\n",
    "                            matched_values = []\n",
    "\n",
    "                            for given_inner_key, given_inner_value in given_inner_schema.items():\n",
    "\n",
    "                                # Check if the json_key is present in the current row\n",
    "                                if given_inner_value in item:\n",
    "                                    \n",
    "                                    matched_values.append({'Query': query_key, 'Col': given_inner_value, 'Value': item[given_inner_value]})\n",
    "                            \n",
    "                            for result in matched_values:\n",
    "                                # Create a DataFrame from the list\n",
    "                                df = pd.DataFrame(result)\n",
    "\n",
    "                        # Convert DataFrame to CSV string\n",
    "                        csv_content = df.to_csv(index=False)\n",
    "\n",
    "                        # Upload to S3\n",
    "                        s3 = boto3.client('s3')\n",
    "\n",
    "                        bucket_name = ''\n",
    "\n",
    "                        # Create the S3 key with the dynamic file prefix\n",
    "                        s3_key = f'eczack_capstone_datalake/staging_zone/serpstack_api/{json_result_type}/{current_date.year}/{current_date.month}/{current_date.day}/{json_result_type}_{timestamp}.csv'\n",
    "\n",
    "                        # Upload CSV content to S3\n",
    "                        s3.put_object(Body=csv_content, Bucket=bucket_name, Key=s3_key)\n",
    "\n",
    "                    else:\n",
    "                        print(f\"I don't know that it is! {json_inner_schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just give it the Keyword, the Country and the Language\n",
    "    \n",
    "# consult1 = serpStackApi().make_api_request('data analysis services', 'us')\n",
    "# save_results_to_csv(consult1)\n",
    "\n",
    "consult2 = [\"data engineering services\", \"data analysis services\", \"mcdonalds\"]\n",
    "# consult2 = [\"mcdonalds\"]\n",
    "location = \"us\"\n",
    "responses = serpStackApi().make_api_request(consult2, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Dictionaries results for query 'data engineering services' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/request/2024/1/18/request_2024-01-18_00-36-21.csv\n",
      "Search_parameters Dictionaries results for query 'data engineering services' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/search_parameters/2024/1/18/search_parameters_2024-01-18_00-36-21.csv\n",
      "Search_information Dictionaries results for query 'data engineering services' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/search_information/2024/1/18/search_information_2024-01-18_00-36-21.csv\n",
      "Organic_results Lists results for query 'data engineering services' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/organic_results/2024/1/18/organic_results_2024-01-18_00-36-21.csv\n",
      "Request Dictionaries results for query 'data analysis services' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/request/2024/1/18/request_2024-01-18_00-36-21.csv\n",
      "Search_parameters Dictionaries results for query 'data analysis services' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/search_parameters/2024/1/18/search_parameters_2024-01-18_00-36-21.csv\n",
      "Search_information Dictionaries results for query 'data analysis services' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/search_information/2024/1/18/search_information_2024-01-18_00-36-21.csv\n",
      "Organic_results Lists results for query 'data analysis services' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/organic_results/2024/1/18/organic_results_2024-01-18_00-36-21.csv\n",
      "Request Dictionaries results for query 'mcdonalds' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/request/2024/1/18/request_2024-01-18_00-36-21.csv\n",
      "Search_parameters Dictionaries results for query 'mcdonalds' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/search_parameters/2024/1/18/search_parameters_2024-01-18_00-36-21.csv\n",
      "Search_information Dictionaries results for query 'mcdonalds' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/search_information/2024/1/18/search_information_2024-01-18_00-36-21.csv\n",
      "Organic_results Lists results for query 'mcdonalds' saved to C:/Users/idbs9/Downloads/eczack_capstone_datalake/staging_zone/organic_results/2024/1/18/organic_results_2024-01-18_00-36-21.csv\n"
     ]
    }
   ],
   "source": [
    "save_results_to_csv(responses)\n",
    "# print(responses)\n",
    "# consult = serpStackApi().make_api_request('mcdonalds', 'us')\n",
    "# print(consult)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
